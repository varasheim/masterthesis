{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#import\n",
    "df = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "df['combined_text'] = df['combined_text'].astype(str)\n",
    "df['tokenized_text'] = df['combined_text'].apply(nltk.word_tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=df['tokenized_text'], vector_size=1000, window=5, min_count=1, workers=4) #train on all texts \n",
    "\n",
    "def get_embedding(tokens):\n",
    "    valid_tokens = [token for token in tokens if token in model.wv.index_to_key]\n",
    "    \n",
    "    if valid_tokens:\n",
    "        return np.mean([model.wv[token] for token in valid_tokens], axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "df['text_embedding'] = df['tokenized_text'].apply(get_embedding)\n",
    "print(df[['combined_text', 'text_embedding']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_0 = df[df['label'] == 0]\n",
    "df_label_1 = df[df['label'] == 1]\n",
    "df_label_2 = df[df['label'] == 2]\n",
    "\n",
    "# Total dataset size and test set calculation\n",
    "total_size = len(df)\n",
    "test_set_size = int(total_size * 0.20)  # 20% of the total dataset for the test set\n",
    "test_samples_label_1_size = int(test_set_size * 0.15)  # 15% of the test set for label 1\n",
    "test_samples_label_2_size = int(test_set_size * 0.05)  # 5% of the test set for label 2\n",
    "\n",
    "test_samples_label_1 = df_label_1.sample(n=test_samples_label_1_size, random_state=42)\n",
    "test_samples_label_2 = df_label_2.sample(n=test_samples_label_2_size, random_state=42)\n",
    "\n",
    "remaining_test_size = test_set_size - (test_samples_label_1_size + test_samples_label_2_size)\n",
    "\n",
    "additional_test_samples = df_label_0.sample(n=remaining_test_size, random_state=42)\n",
    "\n",
    "test_df = pd.concat([test_samples_label_1, test_samples_label_2, additional_test_samples])\n",
    "\n",
    "df_remaining = df.drop(test_df.index)\n",
    "\n",
    "X_train_embeddings = np.stack(df_remaining['text_embedding'].values)\n",
    "y_train = df_remaining['label'].values\n",
    "\n",
    "# test set\n",
    "X_test_embeddings = np.stack(test_df['text_embedding'].values)\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# training dataset\n",
    "X_train, y_train = df_remaining.drop('label', axis=1), df_remaining['label']\n",
    "\n",
    "# Verification of sizes\n",
    "(len(test_df), len(X_train_embeddings), test_samples_label_1_size, test_samples_label_2_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training set\n",
    "import pickle\n",
    "with open('X_train.pkl', 'wb') as file:\n",
    "    pickle.dump(X_train_embeddings, file)\n",
    "\n",
    "with open('y_train.pkl', 'wb') as file:\n",
    "    pickle.dump(y_train, file)\n",
    "\n",
    "with open('X_eval.pkl', 'wb') as file:\n",
    "    pickle.dump(X_test_embeddings, file)\n",
    "\n",
    "with open('y_eval.pkl', 'wb') as file:\n",
    "    pickle.dump(y_test, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pickles for transformers with text not embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_label_0 = df_text[df_text['label'] == 0]\n",
    "df_label_1 = df_text[df_text['label'] == 1]\n",
    "df_label_2 = df_text[df_text['label'] == 2]\n",
    "\n",
    "# Total dataset size and test set calculation\n",
    "total_size = len(df_text)\n",
    "test_set_size = int(total_size * 0.20)  # 20% of the total dataset for the test set\n",
    "test_samples_label_1_size = int(test_set_size * 0.15)  # 15% of the test set for label 1\n",
    "test_samples_label_2_size = int(test_set_size * 0.05)  # 5% of the test set for label 2\n",
    "\n",
    "test_samples_label_1 = df_label_1.sample(n=test_samples_label_1_size, random_state=42)\n",
    "test_samples_label_2 = df_label_2.sample(n=test_samples_label_2_size, random_state=42)\n",
    "\n",
    "remaining_test_size = test_set_size - (test_samples_label_1_size + test_samples_label_2_size)\n",
    "\n",
    "additional_test_samples = df_label_0.sample(n=remaining_test_size, random_state=42)\n",
    "\n",
    "test_df = pd.concat([test_samples_label_1, test_samples_label_2, additional_test_samples])\n",
    "\n",
    "df_remaining = df.drop(test_df.index)\n",
    "\n",
    "X_train_text = np.stack(df_remaining['combined_text'].values)\n",
    "y_train = df_remaining['label'].values\n",
    "\n",
    "# test set\n",
    "X_test_text = np.stack(test_df['combined_text'].values)\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# training dataset\n",
    "X_train, y_train = df_remaining.drop('label', axis=1), df_remaining['label']\n",
    "\n",
    "# Verification of sizes\n",
    "(len(test_df), len(X_train_text), test_samples_label_1_size, test_samples_label_2_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('X_train_text.pkl', 'wb') as file:\n",
    "    pickle.dump(X_train_text, file)\n",
    "\n",
    "with open('y_train_text.pkl', 'wb') as file:\n",
    "    pickle.dump(y_train, file)\n",
    "\n",
    "with open('X_eval_text.pkl', 'wb') as file:\n",
    "    pickle.dump(X_test_text, file)\n",
    "\n",
    "with open('y_eval_text.pkl', 'wb') as file:\n",
    "    pickle.dump(y_test, file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d15c356d5996b53d08cc18227dc02b59f7600461baa871b2d26f243ef3af9c53"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 ('myenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
