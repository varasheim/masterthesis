{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "#load dataset\n",
    "\n",
    "with open('train_smhd_limited.json', 'r') as file:\n",
    "    json_content = file.read()\n",
    "\n",
    "df = pd.read_json(json_content, lines=True)\n",
    "\n",
    "# Use explode to split the 'posts' column into separate rows\n",
    "df_exploded = df.explode('posts', ignore_index=True)\n",
    "\n",
    "df_adhd = df_exploded[df_exploded['condition'] == 'adhd']\n",
    "df_control = df_exploded[df_exploded['condition'] == 'control']\n",
    "\n",
    "control_texts = []\n",
    "for post in df_control['posts']:\n",
    "    control_texts.append(post['text'])\n",
    "\n",
    "adhd_texts = []\n",
    "for p in df_adhd['posts']:\n",
    "    adhd_texts.append(p['text'])\n",
    "\n",
    "adhd_createdat = []\n",
    "for p in df_adhd['posts']:\n",
    "    adhd_createdat.append(p['created_utc'])\n",
    "\n",
    "control_createdat = []\n",
    "for p in df_control['posts']:\n",
    "    control_createdat.append(p['created_utc'])\n",
    "\n",
    "df_adhd['text'] = adhd_texts\n",
    "df_adhd['created_at'] = adhd_createdat\n",
    "df_adhd.drop('posts', axis=1, inplace=True)\n",
    "\n",
    "df_control['text'] = control_texts\n",
    "df_control['created_at'] = control_createdat\n",
    "df_control.drop('posts', axis=1, inplace=True)\n",
    "display(df_control)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_adhd, df_control], ignore_index=True)\n",
    "display(df_combined)\n",
    "\n",
    "df_combined['label_int'] = df_combined['condition'].replace({'control': 0, 'adhd': 1})\n",
    "display(df_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "df_combined['text'] = df_combined['text'].astype(str)\n",
    "\n",
    "df_combined['tokenized_text'] = df_combined['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Write the DataFrame to a JSON file\n",
    "df_combined.to_json('tokenized_data.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenized_data.pkl', 'rb') as file:\n",
    "    tokenized_data = pickle.load(file)\n",
    "\n",
    "df_combined = pd.DataFrame(tokenized_data)\n",
    "display(df_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=df_combined['tokenized_text'], vector_size=1000, window=5, min_count=1, workers=4) #train on all texts \n",
    "\n",
    "def get_embedding(tokens):\n",
    "    valid_tokens = [token for token in tokens if token in model.wv.index_to_key]\n",
    "  \n",
    "    if valid_tokens:\n",
    "        return np.mean([model.wv[token] for token in valid_tokens], axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "df_combined['text_embedding'] = df_combined['tokenized_text'].apply(get_embedding)\n",
    "\n",
    "\n",
    "display(df_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_label_0 = df_combined[df_combined['label_int'] == 0]\n",
    "df_label_1 = df_combined[df_combined['label_int'] == 1]\n",
    "\n",
    "\n",
    "df_label_0 = df_label_0.sample(frac=1, random_state=42)\n",
    "df_label_1 = df_label_1.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "df_80_label_0, df_20_label_0 = train_test_split(df_label_0, test_size=0.2, random_state=42)\n",
    "df_80_label_1, df_20_label_1 = train_test_split(df_label_1, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "df_train = pd.concat([df_80_label_0, df_80_label_1], ignore_index=True)\n",
    "df_test = pd.concat([df_20_label_0, df_20_label_1], ignore_index=True)\n",
    "\n",
    "\n",
    "df_train = df_train.sample(frac=1, random_state=42)\n",
    "df_test = df_test.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['label_int']\n",
    "y_test = df_test['label_int']\n",
    "\n",
    "x_train_embed = df_train['text_embedding']\n",
    "x_test_embed = df_test['text_embedding']\n",
    "\n",
    "x_train_text = df_train['text']\n",
    "x_test_text = df_test['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('x_train_smhd_embed.pkl', 'wb') as file:\n",
    "    pickle.dump(x_train_embed, file)\n",
    "\n",
    "with open('y_train_smhd.pkl', 'wb') as file:\n",
    "    pickle.dump(y_train, file)\n",
    "\n",
    "with open('y_test_smhd.pkl', 'wb') as file:\n",
    "    pickle.dump(y_test, file)\n",
    "\n",
    "with open('x_test_smhd_embed.pkl', 'wb') as file:\n",
    "    pickle.dump(x_test_embed, file)\n",
    "\n",
    "with open('x_test_smhd_text.pkl', 'wb') as file:\n",
    "    pickle.dump(x_test_text, file)\n",
    "\n",
    "with open('x_train_smhd_text.pkl', 'wb') as file:\n",
    "    pickle.dump(x_train_text, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
