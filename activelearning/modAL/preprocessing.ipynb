{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the excel file and printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"ADHD2012.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df_filtered = df[~(df['label'] == 'x')]\n",
    "df_filtered = df[~(df['label'] == 'X')]\n",
    "\n",
    "df_anonymized = df[['title','selftext','label']]\n",
    "df_anonymized = df_anonymized.dropna(subset=['title', 'selftext'])\n",
    "df_anonymized = df_anonymized[(df_anonymized['title'] != '') & (df_anonymized['selftext'] != '')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    # Remove usernames\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "df_anonymized['title'] = df_anonymized['title'].apply(clean_text)\n",
    "df_anonymized['selftext'] = df_anonymized['selftext'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading /Users/varasheim/Desktop/masterny/masterdata\n",
      "[nltk_data]     /activelearning/modAL/embeddings/tokenizers/punkt:\n",
      "[nltk_data]     Package '/Users/varasheim/Desktop/masterny/masterdata/\n",
      "[nltk_data]     activelearning/modAL/embeddings/tokenizers/punkt' not\n",
      "[nltk_data]     found in index\n",
      "[nltk_data] Error loading /Users/hildemikaelsen/Desktop/masterdata/mas\n",
      "[nltk_data]     terdata/activelearning/modAL/embeddings/tokenizers/pun\n",
      "[nltk_data]     kt: Package '/Users/hildemikaelsen/Desktop/masterdata/\n",
      "[nltk_data]     masterdata/activelearning/modAL/embeddings/tokenizers/\n",
      "[nltk_data]     punkt' not found in index\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "df_anonymized['title'] = df_anonymized['title'].astype(str)\n",
    "df_anonymized['selftext'] = df_anonymized['selftext'].astype(str)\n",
    "df_anonymized['label'] = df_anonymized['label'].str.lower()\n",
    "\n",
    "\n",
    "df_combined= pd.DataFrame({\n",
    "    'id': range(1, len(df_anonymized) + 1),\n",
    "    'combined_text': df_anonymized['title'] + df_anonymized['selftext'],\n",
    "    'label': df_anonymized['label']\n",
    "})\n",
    "\n",
    "# Tokenize the 'Text' column\n",
    "df_combined['tokenized_text'] = df_combined['combined_text'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map labels to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'none': 0, 'self-diagnosis': 1, 'self-medication': 2}\n",
    "df_combined['label'] = df_combined['label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id                                      combined_text  \\\n",
      "0          1  Android app to strengthen attention/focusHey /...   \n",
      "1          2  Does anyone here have experience with Imiprami...   \n",
      "3          3  What does the ADHD test look like?I'm 21 and d...   \n",
      "4          4  Are you guys good with maps and directions?It ...   \n",
      "5          5  Just started Concerta today any advice/experie...   \n",
      "...      ...                                                ...   \n",
      "17841  17841  Where are my programmers/coders/developers wit...   \n",
      "17842  17842  Chemically, what is ADHD?I heard its just a la...   \n",
      "17843  17843  Modeling sensitization to stimulants in humans...   \n",
      "17844  17844  How to know when my meds have worn off. AKA: I...   \n",
      "17845  17845  Anyone here taken the Fundamentals of Engineer...   \n",
      "\n",
      "                                          text_embedding  \n",
      "0      [0.055784408, 0.0014394955, 0.0440787, 0.12509...  \n",
      "1      [-0.1229137, 0.22171226, -0.054751925, 0.02254...  \n",
      "3      [-0.1288491, 0.08821578, -0.075796, -0.0226046...  \n",
      "4      [-0.1383845, 0.18256125, -0.11496386, 0.009076...  \n",
      "5      [-0.033305086, 0.15282984, 0.0058798413, 0.109...  \n",
      "...                                                  ...  \n",
      "17841  [-0.21248831, 0.15731336, -0.013198897, -0.027...  \n",
      "17842  [-0.14145045, 0.21852936, -0.042686753, 0.1407...  \n",
      "17843  [-0.050774485, 0.13196814, 0.008879609, 0.0833...  \n",
      "17844  [-0.020260446, 0.081894785, -0.05730386, 0.115...  \n",
      "17845  [-0.009668956, 0.087731406, -0.03834617, 0.047...  \n",
      "\n",
      "[17845 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=df_combined['tokenized_text'], vector_size=1000, window=5, min_count=1, workers=4) #train on all texts \n",
    "\n",
    "def get_embedding(tokens):\n",
    "    valid_tokens = [token for token in tokens if token in model.wv.index_to_key]\n",
    "    \n",
    "    if valid_tokens:\n",
    "        return np.mean([model.wv[token] for token in valid_tokens], axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Apply the function to create title embeddings for all titles in the 'tokenized_title' column\n",
    "df_combined['text_embedding'] = df_combined['tokenized_text'].apply(get_embedding)\n",
    "print(df_combined[['id', 'combined_text', 'text_embedding']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into labeled and unlabeled dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_labeled = df_combined[~df_combined['label'].isna()]\n",
    "embeddings_unlabeled = df_combined[df_combined['label'].isna()]\n",
    "\n",
    "labeled_array = np.array(embeddings_labeled[['label', 'text_embedding']].to_numpy())\n",
    "unlabeled_array = np.array(embeddings_unlabeled[['label', 'text_embedding']].to_numpy())\n",
    "\n",
    "X_pool = np.array([item[1] for item in unlabeled_array])\n",
    "Y_pool = np.array([item[0] for item in unlabeled_array])\n",
    "\n",
    "\n",
    "remove_these = np.random.choice(labeled_array.shape[0], 200, replace=False)\n",
    "evaluation_data = labeled_array[remove_these]\n",
    "labeled_new = np.delete(labeled_array, remove_these, axis=0)\n",
    "\n",
    "X_training = np.array([item[1] for item in labeled_new])\n",
    "y_training = np.array([item[0] for item in labeled_new])\n",
    "\n",
    "evaluation_x= np.array([item[1] for item in evaluation_data])\n",
    "evaluation_y= np.array([item[0] for item in evaluation_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make arrays for transformer models\n",
    "labeled_text_array = np.array(embeddings_labeled[['label', 'combined_text']].to_numpy())\n",
    "unlabeled_text_array = np.array(embeddings_unlabeled[['label', 'combined_text']].to_numpy())\n",
    "\n",
    "X_pool_text = np.array([item[1] for item in unlabeled_text_array])\n",
    "Y_pool_text = np.array([item[0] for item in unlabeled_text_array])\n",
    "\n",
    "\n",
    "remove_these_text = np.random.choice(labeled_text_array.shape[0], 200, replace=False)\n",
    "evaluation_data_text = labeled_text_array[remove_these_text]\n",
    "labeled_new_text = np.delete(labeled_text_array, remove_these_text, axis=0)\n",
    "\n",
    "X_training_text = np.array([item[1] for item in labeled_new_text])\n",
    "y_training_text = np.array([item[0] for item in labeled_new_text])\n",
    "\n",
    "evaluation_x_text= np.array([item[1] for item in evaluation_data_text])\n",
    "evaluation_y_text= np.array([item[0] for item in evaluation_data_text])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save preprocessed arrays with combined text to use for transformer models in pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('x_pool_text.pkl', 'wb') as file:\n",
    "    pickle.dump(X_pool_text, file)\n",
    "\n",
    "with open('y_pool_text.pkl', 'wb') as file:\n",
    "    pickle.dump(Y_pool_text, file)\n",
    "\n",
    "with open('unlabeled_text_array.pkl', 'wb') as file:\n",
    "    pickle.dump(unlabeled_text_array, file)\n",
    "\n",
    "with open('labeled_text_array.pkl', 'wb') as file:\n",
    "    pickle.dump(labeled_text_array, file)\n",
    "\n",
    "with open('X_training_text.pkl', 'wb') as file:\n",
    "    pickle.dump(X_training_text, file)\n",
    "\n",
    "with open('y_training_text.pkl', 'wb') as file:\n",
    "    pickle.dump(y_training_text, file)\n",
    "\n",
    "with open('evaluation_x_text.pkl', 'wb') as file:\n",
    "    pickle.dump(evaluation_x_text, file)\n",
    "\n",
    "with open('evaluation_y_text.pkl', 'wb') as file:\n",
    "    pickle.dump(evaluation_y_text, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save preprocessed arrays with Word2Vec embeddings to use for traditional and deep learning models in pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('x_pool_w2v.pkl', 'wb') as file:\n",
    "    pickle.dump(X_pool, file)\n",
    "\n",
    "with open('y_pool_w2v.pkl', 'wb') as file:\n",
    "    pickle.dump(Y_pool, file)\n",
    "\n",
    "with open('embeddings_unlabeled_w2v.pkl', 'wb') as file:\n",
    "    pickle.dump(embeddings_unlabeled, file)\n",
    "\n",
    "with open('X_training_unbalanced_w2v.pkl', 'wb') as file:\n",
    "    pickle.dump(X_training, file)\n",
    "\n",
    "with open('y_training_unbalanced_w2v.pkl', 'wb') as file:\n",
    "    pickle.dump(y_training, file)\n",
    "\n",
    "with open('evaluation_x_unbalanced_w2v.pkl', 'wb') as file:\n",
    "    pickle.dump(evaluation_x, file)\n",
    "\n",
    "with open('evaluation_y_unbalanced_w2v.pkl', 'wb') as file:\n",
    "    pickle.dump(evaluation_y, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(embeddings_unlabeled['text_embedding'].tolist())\n",
    "predict_test = X_test[:300]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
